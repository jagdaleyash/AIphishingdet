{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagdaleyash/AIphishingdet/blob/main/AI_generated_phishing_detection_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wegstw9HiQB8",
        "outputId": "af1b37d7-245f-461c-d3ce-ec75aef396a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from flask import Flask, request, jsonify\n",
        "import re\n",
        "\n",
        "\n",
        "# Download NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0mdbwBNKa5uw"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stpA9SzUkgCW",
        "outputId": "ef79932a-b589-4ea7-806e-19e1b9ee3164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-512a5212f5e6>:2: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('Phishing_paper1.csv', header=None, names=['url', 'label'])\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('Phishing_paper1.csv', header=None, names=['url', 'label'])\n",
        "\n",
        "# Drop the first row as it contains column names\n",
        "data = data.drop(data.index[0])\n",
        "\n",
        "# Remove rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert label column to numeric values\n",
        "data['label'] = pd.to_numeric(data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tyd2p93akqy9"
      },
      "outputs": [],
      "source": [
        "# Replace NaN values with empty strings\n",
        "data['url'].fillna('', inplace=True)\n",
        "\n",
        "# Convert url column to string\n",
        "data['url'] = data['url'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PjNKcuW_bHJC"
      },
      "outputs": [],
      "source": [
        "# Define function to clean url and detect AI-generated phishing links\n",
        "def clean_url(url):\n",
        "    url = url.lower()\n",
        "    url = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',url)\n",
        "    url = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', 'IPADDRESS', url)\n",
        "    url = re.sub(r'[^\\w\\s]','',url)\n",
        "    url = re.sub(r'\\s+', ' ', url)\n",
        "\n",
        "    # AI detection logic\n",
        "    ai_keywords = ['ai', 'deepfake', 'neural', 'model', 'generated', 'automated']\n",
        "    if any(keyword in url for keyword in ai_keywords):\n",
        "        url = re.sub(r'ai|deepfake|neural|model|generated|automated', 'AIKEYWORD', url)\n",
        "\n",
        "    return url\n",
        "\n",
        "def style_ai_detection(email_content):\n",
        "    ai_patterns = [r'\\!\\!+', r'^[A-Z][a-z]*\\s' * len(email_content.split()) + r'[\\.!?]$']\n",
        "    ai_detected = any(re.search(pattern, email_content) for pattern in ai_patterns)\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "def content_source_analysis(email_content):\n",
        "    reputable_domains = ['nytimes.com', 'bbc.com', 'wikipedia.org']\n",
        "    links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', email_content)\n",
        "\n",
        "    ai_detected = any(domain not in link for link in links for domain in reputable_domains)\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "def linguistic_analysis(email_content):\n",
        "    sentence_tokens = sent_tokenize(email_content)\n",
        "    complex_sentence_threshold = 0.3\n",
        "    complex_sentences = [sentence for sentence in sentence_tokens if len(sentence.split()) > 20]\n",
        "\n",
        "    ai_detected = len(complex_sentences) / len(sentence_tokens) > complex_sentence_threshold\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "\n",
        "# Define function to perform AI content detection\n",
        "def detect_ai_content(email_content):\n",
        "    # Preprocess the email content\n",
        "    email_content = clean_url(email_content)\n",
        "    email_tokens = tokenize_url(email_content)\n",
        "    email_tokens = remove_stop_words(email_tokens)\n",
        "    email_content = ' '.join(email_tokens)\n",
        "\n",
        "    # Check for AI patterns using keywords\n",
        "    ai_keywords = ['ai', 'deepfake', 'neural', 'model', 'generated', 'automated']\n",
        "    ai_presence = any(keyword in email_content for keyword in ai_keywords)\n",
        "\n",
        "    # Additional AI detection strategies\n",
        "    style_ai_detection_result = style_ai_detection(email_content)\n",
        "    content_source_analysis_result = content_source_analysis(email_content)\n",
        "    linguistic_analysis_result = linguistic_analysis(email_content)\n",
        "\n",
        "    # Combine results from different detection strategies\n",
        "    ai_detected = ai_presence or style_ai_detection_result or content_source_analysis_result or linguistic_analysis_result\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "data['url'] = data['url'].apply(clean_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdcE0VdlNnX",
        "outputId": "b50ddcb6-3803-46ae-8d9a-a2dbd81b0000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized URLs shape: (525754, 2121)\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the urls\n",
        "def tokenize_url(url):\n",
        "    tokens = word_tokenize(url)\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['url'].apply(tokenize_url)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "data['tokens'] = data['tokens'].apply(remove_stop_words)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# join the tokens into a single string\n",
        "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# fit and transform the tokenized URLs\n",
        "X = vectorizer.fit_transform(data['tokens'])\n",
        "\n",
        "# print the shape of the vectorized URLs\n",
        "print('Vectorized URLs shape:', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Ds0OOySwbO_F",
        "outputId": "b056e1cf-c7b6-4662-f08b-3d7999185def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (368027, 2121) (368027,)\n",
            "Testing set shape: (157727, 2121) (157727,)\n",
            "Accuracy: 0.9843019901475334\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=10000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# print the shape of the training and testing sets\n",
        "print('Training set shape:', X_train.shape, y_train.shape)\n",
        "print('Testing set shape:', X_test.shape, y_test.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create logistic regression model with a high maximum number of iterations\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# fit the model to the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# predict on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print accuracy score\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# create logistic regression model with a high maximum number of iterations\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# fit the model to the entire preprocessed dataset\n",
        "logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AzaACXH8dift"
      },
      "outputs": [],
      "source": [
        "@app.route(\"/detect_ai_and_predict_phishing\", methods=[\"POST\"])\n",
        "def detect_ai_and_predict_phishing():\n",
        "    data = request.json  # JSON data sent in the request\n",
        "\n",
        "    if \"email_content\" in data:\n",
        "        email_content = data[\"email_content\"]\n",
        "\n",
        "        # Detect AI content\n",
        "        ai_detected = detect_ai_content(email_content)\n",
        "\n",
        "        # Preprocess email for phishing prediction\n",
        "        email = clean_url(email_content)\n",
        "        tokens = tokenize_url(email)\n",
        "        tokens = remove_stop_words(tokens)\n",
        "        email = ' '.join(tokens)\n",
        "        email_vector = vectorizer.transform([email])\n",
        "\n",
        "        # Predict phishing using the trained model\n",
        "        phishing_prediction = logreg.predict(email_vector)\n",
        "\n",
        "        result = {\n",
        "            \"ai_detected\": ai_detected,\n",
        "            \"phishing_prediction\": int(phishing_prediction[0])\n",
        "        }\n",
        "        return jsonify(result)\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Missing 'email_content' in request data\"}), 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnLJJgFUbdb1",
        "outputId": "c9c5276d-d289-43d5-fab4-e6dc0c06715d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This email is not a phishing email\n"
          ]
        }
      ],
      "source": [
        "# preprocess the email\n",
        "email = 'http://example.com'\n",
        "\n",
        "email = clean_url(email)\n",
        "tokens = tokenize_url(email)\n",
        "tokens = remove_stop_words(tokens)\n",
        "email = ' '.join(tokens)\n",
        "\n",
        "# vectorize the preprocessed email\n",
        "email_vector = vectorizer.transform([email])\n",
        "\n",
        "# predict whether the email is phishing or not using the trained model\n",
        "prediction = logreg.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "    print('This email is a phishing email')\n",
        "else:\n",
        "    print('This email is not a phishing email')\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5CbOPMediRx",
        "outputId": "73c06253-a28c-48a4-9cdc-cb68f1d1cbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email content does not indicate AI-generated.\n"
          ]
        }
      ],
      "source": [
        "# Example email content\n",
        "email_content = \"Dear customer, we have detected suspicious activity on your account. Please click on the link to verify your account: http://example.com/verify?id=1kjjk23\"\n",
        "\n",
        "# Detect AI content\n",
        "ai_detected = detect_ai_content(email_content)\n",
        "\n",
        "if ai_detected:\n",
        "    print(\"The email content indicates AI-generated.\")\n",
        "else:\n",
        "    print(\"The email content does not indicate AI-generated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVn8C9VubiAJ",
        "outputId": "4dd7d211-403e-4e32-e6b7-b047a0d3c0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is not a phishing email.\n"
          ]
        }
      ],
      "source": [
        "# example email\n",
        "email = \"Dear customer, we have detected suspicious activity on your account. Please click on the link to verify your account: http://example.com/verify?id=1kjjk23\"\n",
        "\n",
        "# preprocess the email\n",
        "email = clean_url(email)\n",
        "tokens = tokenize_url(email)\n",
        "tokens = remove_stop_words(tokens)\n",
        "email = ' '.join(tokens)\n",
        "\n",
        "# vectorize the email using the trained vectorizer\n",
        "email_vector = vectorizer.transform([email])\n",
        "\n",
        "# predict whether the email is phishing or not using the trained model\n",
        "prediction = logreg.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "    print(\"The email is a phishing email.\")\n",
        "else:\n",
        "    print(\"The email is not a phishing email.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEiA6g62drhu",
        "outputId": "eb6748f2-04ce-4ea2-833f-ce206399d33b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa7j0ABD+75F5Powjs1koM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
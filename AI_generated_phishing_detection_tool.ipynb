{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/lC+LpMESIc68Sh3uaXuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagdaleyash/AIphishingdet/blob/main/AI_generated_phishing_detection_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wegstw9HiQB8",
        "outputId": "55129a8c-b581-4502-db05-2d9426a63283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-22-e5d962f8559e>:15: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  data = pd.read_csv('Phishing_paper1.csv', header=None, names=['url', 'label'])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "# Download NLTK resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('Phishing_paper1.csv', header=None, names=['url', 'label'])\n",
        "\n",
        "# Drop the first row as it contains column names\n",
        "data = data.drop(data.index[0])\n",
        "\n",
        "# Remove rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert label column to numeric values\n",
        "data['label'] = pd.to_numeric(data['label'])\n",
        "\n",
        "# Replace NaN values with empty strings\n",
        "data['url'].fillna('', inplace=True)\n",
        "\n",
        "# Convert url column to string\n",
        "data['url'] = data['url'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to clean url and detect AI-generated phishing links\n",
        "def clean_url(url):\n",
        "    url = url.lower()\n",
        "    url = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',url)\n",
        "    url = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', 'IPADDRESS', url)\n",
        "    url = re.sub(r'[^\\w\\s]','',url)\n",
        "    url = re.sub(r'\\s+', ' ', url)\n",
        "\n",
        "    # AI detection logic\n",
        "    ai_keywords = ['ai', 'deepfake', 'neural', 'model', 'generated', 'automated']\n",
        "    if any(keyword in url for keyword in ai_keywords):\n",
        "        url = re.sub(r'ai|deepfake|neural|model|generated|automated', 'AIKEYWORD', url)\n",
        "\n",
        "    return url\n",
        "\n",
        "def style_ai_detection(email_content):\n",
        "    # Placeholder example: Analyze the email content's use of punctuation and capitalization\n",
        "    # If the content exhibits very formal, overly precise punctuation and capitalization patterns, it might indicate AI\n",
        "    # Note: A more sophisticated analysis would involve training a model on real AI-generated content.\n",
        "\n",
        "    # Sample patterns to detect: excessive use of exclamation marks, formal capitalization of every word, etc.\n",
        "    ai_patterns = [r'\\!\\!+', r'^[A-Z][a-z]*\\s' * len(email_content.split()) + r'[\\.!?]$']\n",
        "    ai_detected = any(re.search(pattern, email_content) for pattern in ai_patterns)\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "def content_source_analysis(email_content):\n",
        "    # Placeholder example: Check if the email content contains links from known reliable sources\n",
        "    # AI-generated content might frequently use obscure or unverified sources\n",
        "\n",
        "    # Sample list of reputable domains\n",
        "    reputable_domains = ['nytimes.com', 'bbc.com', 'wikipedia.org']\n",
        "    links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', email_content)\n",
        "\n",
        "    ai_detected = any(domain not in link for link in links for domain in reputable_domains)\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "def linguistic_analysis(email_content):\n",
        "    # Placeholder example: Check if the email content contains overly complex sentence structures\n",
        "    # AI-generated content might overuse complex and unnatural sentence structures\n",
        "\n",
        "    # Sample indicators: Long sentences, many clauses, excessive semicolons\n",
        "    sentence_tokens = sent_tokenize(email_content)\n",
        "    complex_sentence_threshold = 0.3\n",
        "    complex_sentences = [sentence for sentence in sentence_tokens if len(sentence.split()) > 20]\n",
        "\n",
        "    ai_detected = len(complex_sentences) / len(sentence_tokens) > complex_sentence_threshold\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "\n",
        "# Define function to perform AI content detection\n",
        "def detect_ai_content(email_content):\n",
        "    # Preprocess the email content\n",
        "    email_content = clean_url(email_content)\n",
        "    email_tokens = tokenize_url(email_content)\n",
        "    email_tokens = remove_stop_words(email_tokens)\n",
        "    email_content = ' '.join(email_tokens)\n",
        "\n",
        "    # Check for AI patterns using keywords\n",
        "    ai_keywords = ['ai', 'deepfake', 'neural', 'model', 'generated', 'automated']\n",
        "    ai_presence = any(keyword in email_content for keyword in ai_keywords)\n",
        "\n",
        "    # Additional AI detection strategies\n",
        "    style_ai_detection_result = style_ai_detection(email_content)  # Replace with your style analysis function\n",
        "    content_source_analysis_result = content_source_analysis(email_content)  # Replace with your content source analysis function\n",
        "    linguistic_analysis_result = linguistic_analysis(email_content)  # Replace with your linguistic analysis function\n",
        "\n",
        "    # Combine results from different detection strategies\n",
        "    ai_detected = ai_presence or style_ai_detection_result or content_source_analysis_result or linguistic_analysis_result\n",
        "\n",
        "    return ai_detected\n",
        "\n",
        "data['url'] = data['url'].apply(clean_url)\n",
        "\n",
        "# Tokenize the urls\n",
        "def tokenize_url(url):\n",
        "    tokens = word_tokenize(url)\n",
        "    return tokens\n",
        "\n",
        "data['tokens'] = data['url'].apply(tokenize_url)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "data['tokens'] = data['tokens'].apply(remove_stop_words)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# join the tokens into a single string\n",
        "data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# fit and transform the tokenized URLs\n",
        "X = vectorizer.fit_transform(data['tokens'])\n",
        "\n",
        "# print the shape of the vectorized URLs\n",
        "print('Vectorized URLs shape:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjNKcuW_bHJC",
        "outputId": "bff7eef6-dce3-4e92-a2eb-09923f9ecd79"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized URLs shape: (133803, 1787)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# print the shape of the training and testing sets\n",
        "print('Training set shape:', X_train.shape, y_train.shape)\n",
        "print('Testing set shape:', X_test.shape, y_test.shape)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# create logistic regression model with a high maximum number of iterations\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# fit the model to the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# predict on the testing data\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# print accuracy score\n",
        "print('Accuracy:', accuracy)\n",
        "\n",
        "# create logistic regression model with a high maximum number of iterations\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# fit the model to the entire preprocessed dataset\n",
        "logreg.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Ds0OOySwbO_F",
        "outputId": "fef0a173-e611-4514-9146-b8a119a2bc44"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (93662, 1787) (93662,)\n",
            "Testing set shape: (40141, 1787) (40141,)\n",
            "Accuracy: 0.9374704167808475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=10000)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the email\n",
        "email = 'http://example.com'\n",
        "\n",
        "email = clean_url(email)\n",
        "tokens = tokenize_url(email)\n",
        "tokens = remove_stop_words(tokens)\n",
        "email = ' '.join(tokens)\n",
        "\n",
        "# vectorize the preprocessed email\n",
        "email_vector = vectorizer.transform([email])\n",
        "\n",
        "# predict whether the email is phishing or not using the trained model\n",
        "prediction = logreg.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "    print('This email is a phishing email')\n",
        "else:\n",
        "    print('This email is not a phishing email')\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnLJJgFUbdb1",
        "outputId": "61717556-3175-4798-dcdd-ec9d44d99f11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This email is not a phishing email\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCzikgAscXqv",
        "outputId": "c3ee3166-3082-4221-dad8-244b241e1038"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      1.00      0.97     37631\n",
            "         1.0       0.00      0.00      0.00      2510\n",
            "\n",
            "    accuracy                           0.94     40141\n",
            "   macro avg       0.47      0.50      0.48     40141\n",
            "weighted avg       0.88      0.94      0.91     40141\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example email content\n",
        "email_content = \"Dear customer, we have detected suspicious activity on your account. Please click on the link to verify your account: http://example.com/verify?id=1kjjk23\"\n",
        "\n",
        "# Detect AI content\n",
        "ai_detected = detect_ai_content(email_content)\n",
        "\n",
        "if ai_detected:\n",
        "    print(\"The email content indicates AI-generated.\")\n",
        "else:\n",
        "    print(\"The email content does not indicate AI-generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5CbOPMediRx",
        "outputId": "01463cda-2c6b-438b-e3fe-7e06fe4b4eac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email content does not indicate AI-generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example email\n",
        "email = \"Dear customer, we have detected suspicious activity on your account. Please click on the link to verify your account: http://example.com/verify?id=1kjjk23\"\n",
        "\n",
        "# preprocess the email\n",
        "email = clean_url(email)\n",
        "tokens = tokenize_url(email)\n",
        "tokens = remove_stop_words(tokens)\n",
        "email = ' '.join(tokens)\n",
        "\n",
        "# vectorize the email using the trained vectorizer\n",
        "email_vector = vectorizer.transform([email])\n",
        "\n",
        "# predict whether the email is phishing or not using the trained model\n",
        "prediction = logreg.predict(email_vector)\n",
        "\n",
        "if prediction == 1:\n",
        "    print(\"The email is a phishing email.\")\n",
        "else:\n",
        "    print(\"The email is not a phishing email.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVn8C9VubiAJ",
        "outputId": "be11ac25-a12a-49eb-eb36-6c58db2e547e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is not a phishing email.\n"
          ]
        }
      ]
    }
  ]
}